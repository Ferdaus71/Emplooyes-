# -*- coding: utf-8 -*-
"""Employees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jg8iI1eUd4tYGjaoEk5DgIR3Ndd1dcDB
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import cv2
from PIL import Image
import tempfile
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import base64
import io

# ============================================
# 1. REINFORCEMENT LEARNING MODEL
# ============================================

class RLPolicyNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=128):
        super(RLPolicyNetwork, self).__init__()

        self.state_processor = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        self.action_value = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_size)
        )

        self.state_value = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, state):
        state_features = self.state_processor(state)
        action_values = self.action_value(state_features)
        state_value = self.state_value(state_features)
        return action_values, state_value

class HybridRLAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.learning_rate = learning_rate

        self.policy_net = RLPolicyNetwork(state_size, action_size)
        self.target_net = RLPolicyNetwork(state_size, action_size)
        self.target_net.load_state_dict(self.policy_net.state_dict())

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)

        self.memory = deque(maxlen=10000)

        self.actions = [
            "No action needed",
            "Provide positive reinforcement",
            "Schedule wellness check",
            "Offer skill training",
            "Adjust workload",
            "Recommend promotion track",
            "Provide constructive feedback"
        ]

        self.batch_size = 32
        self.update_target_freq = 100
        self.steps_done = 0

    def select_action(self, state, epsilon=0.1):
        if random.random() < epsilon:
            return random.randint(0, self.action_size - 1)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_values, _ = self.policy_net(state_tensor)
            return torch.argmax(action_values).item()

    def store_experience(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_step(self):
        if len(self.memory) < self.batch_size:
            return 0

        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions).unsqueeze(1)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)

        current_q_values, _ = self.policy_net(states)
        current_q = current_q_values.gather(1, actions)

        with torch.no_grad():
            next_q_values, _ = self.target_net(next_states)
            next_q = next_q_values.max(1)[0].unsqueeze(1)

        target_q = rewards + (1 - dones) * self.gamma * next_q

        loss = nn.MSELoss()(current_q, target_q)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.steps_done += 1
        if self.steps_done % self.update_target_freq == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        return loss.item()

    def get_action_description(self, action_idx):
        if 0 <= action_idx < len(self.actions):
            return self.actions[action_idx]
        return "Unknown action"

    def save_model(self, path):
        torch.save({
            'policy_state_dict': self.policy_net.state_dict(),
            'target_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)

    def load_model(self, path):
        if os.path.exists(path):
            checkpoint = torch.load(path)
            self.policy_net.load_state_dict(checkpoint['policy_state_dict'])
            self.target_net.load_state_dict(checkpoint['target_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            return True
        return False

# ============================================
# 2. DATA MANAGEMENT AND PROCESSING
# ============================================

class EmployeeDatabase:
    def __init__(self):
        self.df = None
        self.employee_data = {}
        self.load_existing_data()

    def load_existing_data(self):
        default_path = "employee_multimodal_dataset.csv"
        if os.path.exists(default_path):
            try:
                self.df = pd.read_csv(default_path)
                self.process_dataframe()
                st.success(f"Loaded existing dataset with {len(self.df)} records")
            except Exception as e:
                st.error(f"Error loading existing dataset: {e}")
                self.df = pd.DataFrame()
        else:
            self.df = pd.DataFrame()

    def process_dataframe(self):
        if self.df is not None and not self.df.empty:
            required_columns = ['Team Members', 'Signed In', 'Signed Out', 'Completed Task']
            for col in required_columns:
                if col not in self.df.columns:
                    self.df[col] = ''

            time_columns = ['Signed In', 'Signed Out']
            for col in time_columns:
                if col in self.df.columns:
                    self.df[col] = pd.to_datetime(self.df[col], errors='coerce').dt.strftime('%I:%M %p')

            self.df = self.df.fillna('')

            self.employee_data = {}
            for idx, row in self.df.iterrows():
                employee_name = str(row.get('Team Members', '')).strip()
                if employee_name:
                    self.employee_data[employee_name] = {
                        'sign_in': row.get('Signed In', ''),
                        'sign_out': row.get('Signed Out', ''),
                        'task': row.get('Completed Task', ''),
                        'date': row.get('Date', ''),
                        'session_id': row.get('session_id', ''),
                        'selfie_path': row.get('selfie_path', ''),
                        'session_video_path': row.get('session_video_path', '')
                    }

    def upload_dataset(self, uploaded_file):
        if uploaded_file is None:
            return "No file uploaded. Please select a CSV file.", pd.DataFrame()

        try:
            self.df = pd.read_csv(uploaded_file)
            self.process_dataframe()
            message = f"Dataset uploaded successfully. Loaded {len(self.df)} employee records."
            return message, self.df

        except Exception as e:
            return f"Error uploading dataset: {str(e)}", pd.DataFrame()

    def search_employees(self, search_term):
        if not search_term or self.df is None or self.df.empty:
            return []

        search_term = search_term.lower().strip()
        matches = []

        if 'Team Members' in self.df.columns:
            mask = self.df['Team Members'].astype(str).str.lower().str.contains(search_term, na=False)
            matches_df = self.df[mask]

            for idx, row in matches_df.iterrows():
                employee_name = str(row['Team Members']).strip()
                if employee_name:
                    matches.append(employee_name)

        matches = list(dict.fromkeys(matches))[:20]
        return matches

    def get_employee_details(self, employee_name):
        if not employee_name or employee_name not in self.employee_data:
            return '', '', '', ''

        details = self.employee_data.get(employee_name, {})
        return (
            details.get('sign_in', ''),
            details.get('sign_out', ''),
            details.get('task', ''),
            details.get('date', '')
        )

    def save_new_record(self, employee_data):
        if self.df is None:
            self.df = pd.DataFrame()

        new_record = {
            'Date': employee_data.get('date', datetime.now().strftime('%d.%m.%Y')),
            'Team Members': employee_data.get('name', ''),
            'Signed In': employee_data.get('sign_in', ''),
            'Signed Out': employee_data.get('sign_out', ''),
            'Completed Task': employee_data.get('task', ''),
            'session_id': len(self.df) + 1 if not self.df.empty else 1,
            'selfie_path': employee_data.get('selfie_path', ''),
            'session_video_path': employee_data.get('session_video_path', ''),
            'emotion_label': '',
            'engagement_level': '',
            'posture_label': '',
            'performance_label': ''
        }

        self.df = pd.concat([self.df, pd.DataFrame([new_record])], ignore_index=True)

        self.employee_data[employee_data.get('name', '')] = {
            'sign_in': employee_data.get('sign_in', ''),
            'sign_out': employee_data.get('sign_out', ''),
            'task': employee_data.get('task', ''),
            'date': employee_data.get('date', '')
        }

        self.save_to_csv()

        return f"Record saved for {employee_data.get('name', '')}"

    def save_to_csv(self):
        if self.df is not None and not self.df.empty:
            self.df.to_csv('employee_multimodal_dataset.csv', index=False)
            st.success("Dataset saved to employee_multimodal_dataset.csv")

# ============================================
# 3. MULTIMODAL FEATURE EXTRACTOR
# ============================================

class MultimodalFeatureExtractor:
    def __init__(self):
        self.feature_cache = {}

    def extract_video_features(self, video_file):
        if video_file is None:
            return np.zeros(10)

        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(video_file.read())
                video_path = tmp_file.name

            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                os.unlink(video_path)
                return np.zeros(10)

            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0

            features = [fps, total_frames, duration]

            frame_count = 0
            motion_values = []

            while frame_count < min(total_frames, 100):
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % 10 == 0:
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    frame_variance = np.var(gray)
                    motion_values.append(frame_variance)

                frame_count += 1

            cap.release()
            os.unlink(video_path)

            if motion_values:
                avg_motion = np.mean(motion_values)
                std_motion = np.std(motion_values)
                max_motion = np.max(motion_values)
                features.extend([avg_motion, std_motion, max_motion])
            else:
                features.extend([0, 0, 0])

            while len(features) < 10:
                features.append(0)

            return np.array(features[:10], dtype=np.float32)

        except Exception as e:
            st.error(f"Video feature extraction error: {e}")
            return np.zeros(10)

    def extract_image_features(self, image_file):
        if image_file is None:
            return np.zeros(8)

        try:
            img = Image.open(image_file)
            img_array = np.array(img)
            height, width = img_array.shape[:2]

            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array

            brightness = np.mean(gray)
            contrast = np.std(gray)

            features = [
                width, height,
                brightness, contrast,
                np.mean(img_array[:,:,0]) if len(img_array.shape) == 3 else 0,
                np.mean(img_array[:,:,1]) if len(img_array.shape) == 3 else 0,
                np.mean(img_array[:,:,2]) if len(img_array.shape) == 3 else 0,
                1 if height > 100 and width > 100 else 0
            ]

            return np.array(features[:8], dtype=np.float32)

        except Exception as e:
            st.error(f"Image feature extraction error: {e}")
            return np.zeros(8)

# ============================================
# 4. HYBRID PERFORMANCE ANALYZER
# ============================================

class HybridPerformanceAnalyzer:
    def __init__(self):
        self.feature_extractor = MultimodalFeatureExtractor()

        state_size = 2 + 10 + 8
        action_size = 7

        self.rl_agent = HybridRLAgent(state_size, action_size)

        if os.path.exists('rl_agent_model.pth'):
            self.rl_agent.load_model('rl_agent_model.pth')
            st.success("Loaded pre-trained RL model")
        else:
            st.info("No pre-trained model found. Using untrained agent.")

    def create_state_vector(self, employee_data, video_file, image_file):
        work_hours = self.calculate_work_hours(
            employee_data.get('sign_in', ''),
            employee_data.get('sign_out', '')
        )

        task = employee_data.get('task', '')
        task_complexity = self.assess_task_complexity(task)

        video_features = self.feature_extractor.extract_video_features(video_file)
        image_features = self.feature_extractor.extract_image_features(image_file)

        state_vector = np.concatenate([
            [work_hours / 24.0, task_complexity],
            video_features,
            image_features
        ])

        return state_vector.astype(np.float32)

    def calculate_reward(self, performance_score, work_hours, action_taken):
        base_reward = performance_score / 100.0

        if work_hours > 12:
            hour_penalty = (work_hours - 12) * 0.1
            base_reward -= hour_penalty
        elif work_hours < 4:
            hour_penalty = (4 - work_hours) * 0.05
            base_reward -= hour_penalty

        action_bonuses = {
            0: 0.0, 1: 0.2, 2: 0.1, 3: 0.15, 4: -0.1, 5: 0.3, 6: 0.05
        }

        bonus = action_bonuses.get(action_taken, 0.0)

        reward = base_reward + bonus
        return np.clip(reward, -1.0, 1.0)

    def analyze_performance(self, employee_data, video_file, image_file, train_agent=False):
        state = self.create_state_vector(employee_data, video_file, image_file)

        epsilon = 0.1 if train_agent else 0.0
        action_idx = self.rl_agent.select_action(state, epsilon)

        work_hours = self.calculate_work_hours(
            employee_data.get('sign_in', ''),
            employee_data.get('sign_out', '')
        )

        task = employee_data.get('task', '')
        task_complexity = self.assess_task_complexity(task)

        video_features = self.feature_extractor.extract_video_features(video_file)
        engagement_score = min(100, (video_features[3] / 1000) * 100) if len(video_features) > 3 else 50

        image_features = self.feature_extractor.extract_image_features(image_file)
        image_quality = min(100, (image_features[3] / 100) * 100) if len(image_features) > 3 else 50

        performance_score = self.calculate_performance_score(
            work_hours, task_complexity, engagement_score, image_quality
        )

        reward = self.calculate_reward(performance_score, work_hours, action_idx)
        next_state = state.copy()

        if train_agent:
            self.rl_agent.store_experience(state, action_idx, reward, next_state, False)
            loss = self.rl_agent.train_step()
            if loss:
                st.info(f"RL Model trained. Loss: {loss:.4f}")

        recommendations = self.generate_hybrid_recommendations(
            performance_score, action_idx, work_hours, engagement_score, image_quality
        )

        charts = self.create_visualizations(
            performance_score, engagement_score, image_quality, work_hours, task_complexity
        )

        report = self.generate_hybrid_report(
            employee_data, performance_score, action_idx,
            engagement_score, image_quality, recommendations
        )

        return {
            'report': report,
            'performance_score': performance_score,
            'action_idx': action_idx,
            'action_description': self.rl_agent.get_action_description(action_idx),
            'engagement_score': engagement_score,
            'image_quality': image_quality,
            'work_hours': work_hours,
            'recommendations': recommendations,
            'charts': charts,
            'rl_reward': reward,
            'rl_state': state.tolist()
        }

    def calculate_work_hours(self, sign_in, sign_out):
        try:
            if not sign_in or not sign_out:
                return 8.0

            in_time = datetime.strptime(sign_in, "%H:%M")
            out_time = datetime.strptime(sign_out, "%H:%M")

            duration = (out_time - in_time).seconds / 3600

            if duration < 0:
                duration += 24

            return round(duration, 2)

        except:
            return 8.0

    def assess_task_complexity(self, task_description):
        if not task_description:
            return 0.5

        task_lower = task_description.lower()
        complexity = 0.5

        complexity_keywords = {
            'design': 0.8, 'develop': 0.9, 'implement': 0.85,
            'create': 0.7, 'build': 0.75, 'analyze': 0.6,
            'review': 0.5, 'update': 0.4, 'fix': 0.3, 'test': 0.4
        }

        for keyword, score in complexity_keywords.items():
            if keyword in task_lower:
                complexity = max(complexity, score)

        return complexity

    def calculate_performance_score(self, work_hours, task_complexity, engagement, image_quality):
        weights = {
            'work_hours': 0.25,
            'task_complexity': 0.25,
            'engagement': 0.30,
            'image_quality': 0.20
        }

        hours_score = min(100, (work_hours / 8) * 100) if work_hours <= 12 else 100

        score = (
            hours_score * weights['work_hours'] +
            task_complexity * 100 * weights['task_complexity'] +
            engagement * weights['engagement'] +
            image_quality * weights['image_quality']
        )

        return round(min(100, score), 1)

    def generate_hybrid_recommendations(self, performance_score, action_idx, work_hours, engagement, image_quality):
        recommendations = []

        action_descriptions = {
            0: "Maintain current management approach.",
            1: "Provide specific positive feedback on recent achievements.",
            2: "Schedule a wellness discussion and consider work-life balance adjustments.",
            3: "Identify skill gaps and provide targeted training opportunities.",
            4: "Review current workload and consider redistribution if necessary.",
            5: "Discuss career progression and development opportunities.",
            6: "Provide constructive feedback with specific improvement areas."
        }

        recommendations.append(f"RL Recommended Action: {action_descriptions.get(action_idx, 'Monitor performance')}")

        if performance_score >= 85:
            recommendations.append("Excellent overall performance. Consider leadership opportunities.")
        elif performance_score >= 70:
            recommendations.append("Good performance. Focus on skill development.")
        elif performance_score >= 50:
            recommendations.append("Average performance. Identify improvement areas.")
        else:
            recommendations.append("Performance needs immediate attention.")

        if work_hours > 10:
            recommendations.append("High work hours detected. Monitor for burnout risk.")
        elif work_hours < 6:
            recommendations.append("Low work hours. Assess task allocation and engagement.")

        if engagement < 40:
            recommendations.append("Low engagement detected. Review task interest and motivation.")

        if image_quality < 40:
            recommendations.append("Poor image quality may indicate workspace issues.")

        return recommendations

    def create_visualizations(self, performance_score, engagement, image_quality, work_hours, task_complexity):
        charts = {}

        metrics = ['Performance', 'Engagement', 'Image Quality', 'Work Hours', 'Task Complexity']
        values = [
            performance_score,
            engagement,
            image_quality,
            min(100, (work_hours / 8) * 100),
            task_complexity * 100
        ]

        fig_radar = go.Figure(data=go.Scatterpolar(
            r=values,
            theta=metrics,
            fill='toself',
            line=dict(color='blue', width=2)
        ))

        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
            showlegend=False,
            title="Performance Metrics Radar",
            width=500,
            height=400
        )

        charts['radar'] = fig_radar

        actions = ['No Action', 'Positive', 'Wellness', 'Training', 'Workload', 'Promotion', 'Feedback']
        action_values = [0] * 7

        if performance_score > 80:
            action_values[1] = 1
            action_values[5] = 0.8
        elif performance_score > 60:
            action_values[3] = 1
            action_values[6] = 0.5
        else:
            action_values[2] = 1
            action_values[6] = 1

        fig_bar = go.Figure(data=[go.Bar(
            x=actions,
            y=action_values,
            marker_color=['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728', '#9467bd', '#8c564b', '#e377c2']
        )])

        fig_bar.update_layout(
            title="RL Action Distribution",
            xaxis_title="Action Type",
            yaxis_title="Probability",
            height=400
        )

        charts['bar'] = fig_bar

        return charts

    def generate_hybrid_report(self, employee_data, performance_score, action_idx,
                               engagement_score, image_quality, recommendations):
        report = f"""
        HYBRID RL-BASED PERFORMANCE ANALYSIS REPORT
        {'=' * 60}

        Employee Information:
        - Name: {employee_data.get('name', 'N/A')}
        - Date: {employee_data.get('date', 'N/A')}
        - Task: {employee_data.get('task', 'N/A')}
        - Work Hours: {self.calculate_work_hours(employee_data.get('sign_in', ''), employee_data.get('sign_out', ''))}

        Performance Analysis:
        - Overall Score: {performance_score}/100
        - Performance Level: {'Excellent' if performance_score >= 85 else 'Good' if performance_score >= 70 else 'Average' if performance_score >= 50 else 'Needs Improvement'}

        Multimodal Analysis:
        - Engagement Score: {engagement_score:.1f}/100
        - Image Quality Score: {image_quality:.1f}/100

        Reinforcement Learning Analysis:
        - Recommended Action: {self.rl_agent.get_action_description(action_idx)}
        - Action ID: {action_idx}

        Recommendations:
        """

        for i, rec in enumerate(recommendations, 1):
            report += f"{i}. {rec}\n"

        report += f"\n{'=' * 60}"
        report += f"\nGenerated using Hybrid RL Model"
        report += f"\nReport Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        return report

    def train_on_dataset(self, dataset_path, epochs=5):
        if not os.path.exists(dataset_path):
            return "Dataset not found for training"

        try:
            df = pd.read_csv(dataset_path)
            progress_bar = st.progress(0)

            for epoch in range(epochs):
                epoch_loss = 0
                num_samples = 0

                for idx, row in df.iterrows():
                    employee_data = {
                        'name': row.get('Team Members', ''),
                        'date': row.get('Date', ''),
                        'sign_in': row.get('Signed In', '09:00'),
                        'sign_out': row.get('Signed Out', '17:00'),
                        'task': row.get('Completed Task', '')
                    }

                    work_hours = self.calculate_work_hours(
                        employee_data.get('sign_in', ''),
                        employee_data.get('sign_out', '')
                    )

                    task_complexity = self.assess_task_complexity(employee_data.get('task', ''))

                    video_features = np.random.randn(10) * 0.5 + 0.5
                    image_features = np.random.randn(8) * 0.5 + 0.5

                    state = np.concatenate([
                        [work_hours / 24.0, task_complexity],
                        video_features,
                        image_features
                    ]).astype(np.float32)

                    performance_score = np.random.uniform(30, 95)
                    action_idx = self.rl_agent.select_action(state, epsilon=0.3)
                    reward = self.calculate_reward(performance_score, work_hours, action_idx)

                    next_state = state.copy() + np.random.randn(len(state)) * 0.1

                    self.rl_agent.store_experience(state, action_idx, reward, next_state, False)
                    loss = self.rl_agent.train_step()

                    if loss:
                        epoch_loss += loss
                        num_samples += 1

                if num_samples > 0:
                    avg_loss = epoch_loss / num_samples
                    st.info(f"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}")

                progress_bar.progress((epoch + 1) / epochs)

            self.rl_agent.save_model('rl_agent_model.pth')
            return f"Training completed. Model saved. Average loss: {avg_loss:.4f}"

        except Exception as e:
            return f"Training error: {str(e)}"

# ============================================
# 5. STREAMLIT APPLICATION
# ============================================

def main():
    st.set_page_config(
        page_title="Hybrid RL Employee Performance Analyzer",
        page_icon="ðŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Initialize session state
    if 'database' not in st.session_state:
        st.session_state.database = EmployeeDatabase()

    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = HybridPerformanceAnalyzer()

    if 'current_employee' not in st.session_state:
        st.session_state.current_employee = None

    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None

    # Custom CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2ca02c;
        margin-top: 1rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .recommendation-box {
        background-color: #e8f4f8;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        border-left: 4px solid #1f77b4;
    }
    </style>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("<div class='main-header'>Hybrid RL Employee Performance Analyzer</div>", unsafe_allow_html=True)
        st.markdown("---")

        st.markdown("### Navigation")
        page = st.radio(
            "Select Page",
            ["Dashboard", "Dataset Management", "Performance Analysis", "RL Model Training", "Reports"]
        )

        st.markdown("---")
        st.markdown("### System Status")

        if st.session_state.database.df is not None:
            st.info(f"ðŸ“Š Dataset: {len(st.session_state.database.df)} records loaded")
        else:
            st.warning("ðŸ“Š No dataset loaded")

        if os.path.exists('rl_agent_model.pth'):
            st.success("ðŸ¤– RL Model: Pre-trained model loaded")
        else:
            st.warning("ðŸ¤– RL Model: No pre-trained model found")

        st.markdown("---")
        st.markdown("### Quick Actions")

        if st.button("Clear All Data", type="secondary"):
            st.session_state.analysis_results = None
            st.session_state.current_employee = None
            st.rerun()

        if st.button("Export Dataset"):
            if st.session_state.database.df is not None and not st.session_state.database.df.empty:
                csv = st.session_state.database.df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name="employee_dataset_export.csv",
                    mime="text/csv"
                )

    # Main content based on selected page
    if page == "Dashboard":
        display_dashboard()
    elif page == "Dataset Management":
        display_dataset_management()
    elif page == "Performance Analysis":
        display_performance_analysis()
    elif page == "RL Model Training":
        display_rl_training()
    elif page == "Reports":
        display_reports()

def display_dashboard():
    st.markdown("<div class='main-header'>Dashboard Overview</div>", unsafe_allow_html=True)

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Total Employees",
                 len(st.session_state.database.df) if st.session_state.database.df is not None else 0)

    with col2:
        avg_score = 0
        if st.session_state.analysis_results:
            avg_score = st.session_state.analysis_results.get('performance_score', 0)
        st.metric("Avg Performance", f"{avg_score:.1f}/100")

    with col3:
        st.metric("RL Model Status",
                 "Trained" if os.path.exists('rl_agent_model.pth') else "Untrained")

    with col4:
        st.metric("Last Analysis",
                 datetime.now().strftime("%H:%M") if st.session_state.analysis_results else "N/A")

    st.markdown("---")

    # Recent Analysis
    st.markdown("<div class='sub-header'>Recent Performance Analysis</div>", unsafe_allow_html=True)

    if st.session_state.analysis_results:
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### Performance Metrics")
            st.plotly_chart(st.session_state.analysis_results['charts']['radar'], use_container_width=True)

        with col2:
            st.markdown("#### RL Action Analysis")
            st.plotly_chart(st.session_state.analysis_results['charts']['bar'], use_container_width=True)

        # Key metrics
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
            st.metric("Performance Score", f"{st.session_state.analysis_results['performance_score']:.1f}")
            st.markdown("</div>", unsafe_allow_html=True)

        with col2:
            st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
            st.metric("Engagement", f"{st.session_state.analysis_results['engagement_score']:.1f}")
            st.markdown("</div>", unsafe_allow_html=True)

        with col3:
            st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
            st.metric("Work Hours", f"{st.session_state.analysis_results['work_hours']:.1f}")
            st.markdown("</div>", unsafe_allow_html=True)

        with col4:
            st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
            st.metric("RL Reward", f"{st.session_state.analysis_results['rl_reward']:.3f}")
            st.markdown("</div>", unsafe_allow_html=True)

        # Recommendations
        st.markdown("<div class='sub-header'>Top Recommendations</div>", unsafe_allow_html=True)
        recommendations = st.session_state.analysis_results.get('recommendations', [])
        for rec in recommendations[:3]:
            st.markdown(f"<div class='recommendation-box'>âœ“ {rec}</div>", unsafe_allow_html=True)

    else:
        st.info("No analysis results available. Go to 'Performance Analysis' page to analyze an employee.")

def display_dataset_management():
    st.markdown("<div class='main-header'>Dataset Management</div>", unsafe_allow_html=True)

    # Upload section
    st.markdown("<div class='sub-header'>Upload Dataset</div>", unsafe_allow_html=True)

    uploaded_file = st.file_uploader("Upload CSV file", type=['csv'])

    if uploaded_file is not None:
        if st.button("Process Dataset"):
            with st.spinner("Processing dataset..."):
                message, df = st.session_state.database.upload_dataset(uploaded_file)
                st.success(message)

    # Dataset preview
    st.markdown("<div class='sub-header'>Dataset Preview</div>", unsafe_allow_html=True)

    if st.session_state.database.df is not None and not st.session_state.database.df.empty:
        st.dataframe(st.session_state.database.df.head(20), use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            st.info(f"Total Records: {len(st.session_state.database.df)}")
        with col2:
            st.info(f"Columns: {', '.join(st.session_state.database.df.columns.tolist()[:5])}...")

        # Employee search
        st.markdown("<div class='sub-header'>Employee Search</div>", unsafe_allow_html=True)

        search_term = st.text_input("Search employee by name")

        if search_term:
            matches = st.session_state.database.search_employees(search_term)

            if matches:
                selected_employee = st.selectbox("Select employee", matches)

                if selected_employee:
                    sign_in, sign_out, task, date = st.session_state.database.get_employee_details(selected_employee)

                    col1, col2 = st.columns(2)
                    with col1:
                        st.text_input("Sign In", value=sign_in, disabled=True)
                        st.text_input("Task", value=task, disabled=True)
                    with col2:
                        st.text_input("Sign Out", value=sign_out, disabled=True)
                        st.text_input("Date", value=date, disabled=True)

                    if st.button("Use for Analysis"):
                        st.session_state.current_employee = selected_employee
                        st.success(f"Employee {selected_employee} selected for analysis")
            else:
                st.warning("No employees found matching the search term")
    else:
        st.warning("No dataset loaded. Please upload a CSV file first.")

def display_performance_analysis():
    st.markdown("<div class='main-header'>Performance Analysis</div>", unsafe_allow_html=True)

    # Employee information form
    st.markdown("<div class='sub-header'>Employee Information</div>", unsafe_allow_html=True)

    with st.form("employee_form"):
        col1, col2 = st.columns(2)

        with col1:
            employee_name = st.text_input("Employee Name",
                                        value=st.session_state.current_employee if st.session_state.current_employee else "")
            analysis_date = st.text_input("Analysis Date",
                                        value=datetime.now().strftime("%d.%m.%Y"))

        with col2:
            # 24-hour time selection
            sign_in_time = st.selectbox("Sign In Time",
                                       [f"{h:02d}:{m:02d}" for h in range(0, 24) for m in [0, 30]],
                                       index=18)  # 09:00

            sign_out_time = st.selectbox("Sign Out Time",
                                        [f"{h:02d}:{m:02d}" for h in range(0, 24) for m in [0, 30]],
                                        index=34)  # 17:00

        completed_task = st.text_area("Completed Task",
                                    placeholder="Describe the completed task...",
                                    height=100)

        # Multimedia upload
        st.markdown("<div class='sub-header'>Multimedia Data</div>", unsafe_allow_html=True)

        col1, col2 = st.columns(2)

        with col1:
            session_video = st.file_uploader("Upload Session Video", type=['mp4', 'avi', 'mov'])
            if session_video:
                st.info(f"Video uploaded: {session_video.name}")

        with col2:
            selfie_image = st.file_uploader("Upload Selfie/Workspace Image", type=['jpg', 'jpeg', 'png'])
            if selfie_image:
                st.image(selfie_image, caption="Uploaded Image", width=200)

        # RL training option
        train_mode = st.checkbox("Train RL Model on this analysis", value=False)

        # Submit button
        col1, col2, col3 = st.columns(3)
        with col2:
            analyze_button = st.form_submit_button("ðŸš€ Analyze Performance", type="primary")

        if analyze_button:
            if not employee_name:
                st.error("Please enter employee name")
            else:
                with st.spinner("Analyzing performance with Hybrid RL..."):
                    employee_data = {
                        'name': employee_name,
                        'date': analysis_date,
                        'sign_in': sign_in_time,
                        'sign_out': sign_out_time,
                        'task': completed_task
                    }

                    st.session_state.analysis_results = st.session_state.analyzer.analyze_performance(
                        employee_data, session_video, selfie_image, train_mode
                    )

                    # Save record
                    if st.button("Save Record to Dataset"):
                        employee_data['selfie_path'] = selfie_image.name if selfie_image else ''
                        employee_data['session_video_path'] = session_video.name if session_video else ''
                        result = st.session_state.database.save_new_record(employee_data)
                        st.success(result)

                    st.rerun()

    # Display results if available
    if st.session_state.analysis_results:
        st.markdown("<div class='sub-header'>Analysis Results</div>", unsafe_allow_html=True)

        # Performance metrics
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Performance Score",
                     f"{st.session_state.analysis_results['performance_score']:.1f}",
                     delta="Good" if st.session_state.analysis_results['performance_score'] > 70 else "Needs Improvement")

        with col2:
            st.metric("RL Recommended Action",
                     st.session_state.analysis_results['action_description'])

        with col3:
            st.metric("Engagement Score",
                     f"{st.session_state.analysis_results['engagement_score']:.1f}")

        with col4:
            st.metric("Work Hours",
                     f"{st.session_state.analysis_results['work_hours']:.1f}")

        # Visualizations
        col1, col2 = st.columns(2)

        with col1:
            st.plotly_chart(st.session_state.analysis_results['charts']['radar'], use_container_width=True)

        with col2:
            st.plotly_chart(st.session_state.analysis_results['charts']['bar'], use_container_width=True)

        # Recommendations
        st.markdown("<div class='sub-header'>Recommendations</div>", unsafe_allow_html=True)

        recommendations = st.session_state.analysis_results.get('recommendations', [])
        for rec in recommendations:
            st.markdown(f"<div class='recommendation-box'>â€¢ {rec}</div>", unsafe_allow_html=True)

        # Detailed report
        st.markdown("<div class='sub-header'>Detailed Report</div>", unsafe_allow_html=True)

        with st.expander("View Full Report"):
            st.text(st.session_state.analysis_results['report'])

            # Export report
            if st.button("Export Report as Text"):
                report_text = st.session_state.analysis_results['report']
                st.download_button(
                    label="Download Report",
                    data=report_text,
                    file_name=f"performance_report_{employee_name}_{datetime.now().strftime('%Y%m%d')}.txt",
                    mime="text/plain"
                )

def display_rl_training():
    st.markdown("<div class='main-header'>RL Model Training</div>", unsafe_allow_html=True)

    st.markdown("""
    ### Reinforcement Learning Model Training

    Train the Deep Q-Network RL model on your historical employee data.
    The model learns optimal management actions based on performance patterns.
    """)

    # Training controls
    col1, col2 = st.columns(2)

    with col1:
        epochs = st.slider("Training Epochs", min_value=1, max_value=20, value=5)

    with col2:
        dataset_path = st.selectbox(
            "Training Dataset",
            ["employee_multimodal_dataset.csv", "Upload custom dataset"]
        )

    if dataset_path == "Upload custom dataset":
        custom_dataset = st.file_uploader("Upload training dataset", type=['csv'])
        if custom_dataset:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:
                tmp_file.write(custom_dataset.read())
                dataset_path = tmp_file.name

    if st.button("Start Training", type="primary"):
        if not os.path.exists(dataset_path):
            st.error(f"Dataset not found: {dataset_path}")
        else:
            with st.spinner("Training RL model..."):
                result = st.session_state.analyzer.train_on_dataset(dataset_path, epochs)
                st.success(result)

    # Model information
    st.markdown("<div class='sub-header'>Model Information</div>", unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:
        st.info("""
        **Model Architecture:**
        - Deep Q-Network (DQN)
        - 20 input features
        - 2 hidden layers (128 neurons each)
        - 7 output actions
        - Experience replay buffer
        - Target network
        """)

    with col2:
        st.info("""
        **Training Parameters:**
        - Learning rate: 0.001
        - Gamma (discount): 0.95
        - Batch size: 32
        - Epsilon (exploration): 0.1
        - Target update frequency: 100 steps
        """)

    # Model actions
    st.markdown("<div class='sub-header'>Available Actions</div>", unsafe_allow_html=True)

    actions = st.session_state.analyzer.rl_agent.actions
    for i, action in enumerate(actions):
        st.markdown(f"**{i}.** {action}")

def display_reports():
    st.markdown("<div class='main-header'>Reports & Analytics</div>", unsafe_allow_html=True)

    if st.session_state.database.df is None or st.session_state.database.df.empty:
        st.warning("No data available for reports. Please upload a dataset first.")
        return

    # Generate summary statistics
    st.markdown("<div class='sub-header'>Dataset Statistics</div>", unsafe_allow_html=True)

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Total Employees", len(st.session_state.database.df))

    with col2:
        unique_employees = st.session_state.database.df['Team Members'].nunique()
        st.metric("Unique Employees", unique_employees)

    with col3:
        avg_hours = 8.0  # Default, would need calculation
        st.metric("Avg Work Hours", f"{avg_hours:.1f}")

    with col4:
        st.metric("Total Tasks", len(st.session_state.database.df))

    # Data visualization
    st.markdown("<div class='sub-header'>Data Visualization</div>", unsafe_allow_html=True)

    col1, col2 = st.columns(2)

    with col1:
        # Employee distribution
        if 'Team Members' in st.session_state.database.df.columns:
            employee_counts = st.session_state.database.df['Team Members'].value_counts().head(10)
            fig1 = px.bar(
                x=employee_counts.index,
                y=employee_counts.values,
                title="Top 10 Employees by Records",
                labels={'x': 'Employee', 'y': 'Number of Records'}
            )
            st.plotly_chart(fig1, use_container_width=True)

    with col2:
        # Task type distribution
        if 'Completed Task' in st.session_state.database.df.columns:
            tasks = st.session_state.database.df['Completed Task'].astype(str)
            task_types = tasks.apply(lambda x: x.split()[0] if len(x.split()) > 0 else 'Unknown')
            task_counts = task_types.value_counts().head(8)
            fig2 = px.pie(
                values=task_counts.values,
                names=task_counts.index,
                title="Task Type Distribution"
            )
            st.plotly_chart(fig2, use_container_width=True)

    # Export options
    st.markdown("<div class='sub-header'>Export Options</div>", unsafe_allow_html=True)

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("Export Full Dataset"):
            csv = st.session_state.database.df.to_csv(index=False)
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name="employee_full_dataset.csv",
                mime="text/csv"
            )

    with col2:
        if st.button("Export Summary Report"):
            summary = generate_summary_report(st.session_state.database.df)
            st.download_button(
                label="Download Summary",
                data=summary,
                file_name="employee_summary_report.txt",
                mime="text/plain"
            )

    with col3:
        if st.button("Export RL Model"):
            if os.path.exists('rl_agent_model.pth'):
                with open('rl_agent_model.pth', 'rb') as f:
                    model_data = f.read()
                st.download_button(
                    label="Download Model",
                    data=model_data,
                    file_name="rl_agent_model.pth",
                    mime="application/octet-stream"
                )
            else:
                st.warning("No RL model found")

def generate_summary_report(df):
    """Generate summary report of the dataset"""
    report = f"""
    EMPLOYEE DATASET SUMMARY REPORT
    {'=' * 50}

    Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    Total Records: {len(df)}

    DATASET OVERVIEW:
    - Total Employees: {df['Team Members'].nunique() if 'Team Members' in df.columns else 'N/A'}
    - Date Range: {df['Date'].min() if 'Date' in df.columns else 'N/A'} to {df['Date'].max() if 'Date' in df.columns else 'N/A'}

    COLUMNS ANALYSIS:
    """

    for col in df.columns:
        report += f"- {col}: {df[col].notna().sum()} non-null values\n"

    report += f"\n{'=' * 50}\n"
    return report

if __name__ == "__main__":
    main()